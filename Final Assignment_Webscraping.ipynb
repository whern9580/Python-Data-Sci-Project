{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<center>\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n</center>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1>Extracting Stock Data Using a Web Scraping</h1>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Not all stock data is available via API in this assignment; you will use web-scraping to obtain financial data. You will be quizzed on your results.\\\nUsing beautiful soup we will extract historical share data from a web-page.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>Table of Contents</h2>\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <ul>\n        <li>Downloading the Webpage Using Requests Library</li>\n        <li>Parsing Webpage HTML Using BeautifulSoup</li>\n        <li>Extracting Data and Building DataFrame</li>\n    </ul>\n<p>\n    Estimated Time Needed: <strong>30 min</strong></p>\n</div>\n\n<hr>\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "#!pip install pandas==1.3.3\n#!pip install requests==2.26.0\n!mamba install bs4==4.10.0 -y\n!mamba install html5lib==1.1 -y\n!pip install lxml==4.6.4\n#!pip install plotly==5.3.1"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Using Webscraping to Extract Stock Data Example\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "First we must use the `request` library to downlaod the webpage, and extract the text. We will extract Netflix stock data <https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html>.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/netflix_data_webpage.html\"\n\ndata  = requests.get(url).text"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Next we must parse the text into html using `beautiful_soup`\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "soup = BeautifulSoup(data, 'html5lib')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we can turn the html table into a pandas dataframe\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "netflix_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n\n# First we isolate the body of the table which contains all the information\n# Then we loop through each row and find all the column values for each row\nfor row in soup.find(\"tbody\").find_all('tr'):\n    col = row.find_all(\"td\")\n    date = col[0].text\n    Open = col[1].text\n    high = col[2].text\n    low = col[3].text\n    close = col[4].text\n    adj_close = col[5].text\n    volume = col[6].text\n    \n    # Finally we append the data of each row to the table\n    netflix_data = netflix_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj Close\":adj_close, \"Volume\":volume}, ignore_index=True)    "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can now print out the dataframe\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "netflix_data.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can also use the pandas `read_html` function using the url\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "read_html_pandas_data = pd.read_html(url)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Or we can convert the BeautifulSoup object to a string\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "read_html_pandas_data = pd.read_html(str(soup))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Beacause there is only one table on the page, we just take the first table in the list returned\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "netflix_dataframe = read_html_pandas_data[0]\n\nnetflix_dataframe.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Using Webscraping to Extract Stock Data Exercise\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Use the `requests` library to download the webpage <https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html>. Save the text of the response as a variable named `html_data`.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pandas in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.3.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas) (2021.3)\nRequirement already satisfied: numpy>=1.17.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas) (1.21.4)\nRequirement already satisfied: six>=1.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\nRequirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.26.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (1.26.7)\nRequirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (2.0.8)\nCollecting bs4\n  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting beautifulsoup4\n  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 5.0 MB/s             \n\u001b[?25hCollecting soupsieve>1.2\n  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\nBuilding wheels for collected packages: bs4\n  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1271 sha256=42c6c969c332adc7f63288c78fb20930a8854cf3a99a97b87eb9e2073acc3e11\n  Stored in directory: /home/jupyterlab/.cache/pip/wheels/0a/9e/ba/20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\nSuccessfully built bs4\nInstalling collected packages: soupsieve, beautifulsoup4, bs4\nSuccessfully installed beautifulsoup4-4.10.0 bs4-0.0.1 soupsieve-2.3.1\nCollecting html5lib\n  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112 kB 21.8 MB/s            \n\u001b[?25hRequirement already satisfied: webencodings in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from html5lib) (0.5.1)\nRequirement already satisfied: six>=1.9 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from html5lib) (1.16.0)\nInstalling collected packages: html5lib\nSuccessfully installed html5lib-1.1\n"
                }
            ],
            "source": "!pip install pandas\n!pip install requests\n!pip install bs4\n!pip install html5lib\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Parse the html data using `beautiful_soup`.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'html5lib'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_67/1015061096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhtml5lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'html5lib'"
                    ]
                }
            ],
            "source": "import pandas as pd\nimport requests\nimport html5lib\nfrom bs4 import BeautifulSoup\n\nurl = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-PY0220EN-SkillsNetwork/labs/project/amazon_data_webpage.html\"\n\nhtmldata  = requests.get(url).text\n\nsoup = BeautifulSoup(htmldata, 'html5lib')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Question 1</b> What is the content of the title attribute:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Using beautiful soup extract the table with historical share prices and store it into a dataframe named `amazon_data`. The dataframe should have columns Date, Open, High, Low, Close, Adj Close, and Volume. Fill in each variable with the correct data from the list `col`.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "amazon_data = pd.DataFrame(columns=[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n\nfor row in soup.find(\"tbody\").find_all(\"tr\"):\n    col = row.find_all(\"td\")\n    date = #ADD_CODE\n    Open = #ADD_CODE\n    high = #ADD_CODE\n    low = #ADD_CODE\n    close = #ADD_CODE\n    adj_close = #ADD_CODE\n    volume = #ADD_CODE\n    \n    amazon_data = amazon_data.append({\"Date\":date, \"Open\":Open, \"High\":high, \"Low\":low, \"Close\":close, \"Adj Close\":adj_close, \"Volume\":volume}, ignore_index=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Print out the first five rows of the `amazon_data` dataframe you created.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Question 2</b> What is the name of the columns of the dataframe\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Question 3</b> What is the `Open` of the last row of the amazon_data dataframe?\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>About the Authors:</h2> \n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkPY0220ENSkillsNetwork23455606-2021-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n\nAzim Hirjani\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Change Log\n\n| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n| ----------------- | ------- | ---------- | ------------------ |\n\n```\n| 2021-06-09       | 1.2     | Lakshmi Holla|Added URL in question 3 |\n```\n\n\\| 2020-11-10        | 1.1     | Malika Singla | Deleted the Optional part |\n\\| 2020-08-27        | 1.0     | Malika Singla | Added lab to GitLab       |\n\n<hr>\n\n## <h3 align=\"center\"> \u00a9 IBM Corporation 2020. All rights reserved. <h3/>\n\n<p>\n"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.9",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}